---
title: "[AI] 機器學習的基礎技術"
date: 2024-12-06T16:26:30+08:00
tags: ["AI"]
draft: false
Categories: AI
description: "The fundamentals of Machine Learning"
author: "Rain Hu"
showToc: true
TocOpen: true
math: true
mermaid: true
hidemeta: false
canonicalURL: "https://intervalrain.github.io/"
disableHLJS: true
disableShare: true
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowCodeCopyButtons: true
---

## 機率建模(Probabilistic modeling)

### 單純貝氏演算法(Naive Bayes thorem)
單純貝氏定理是一種基於機率理論的分類方法，適用於文本分類等任務，其核心概念是基於特徵條件獨立的假設來計算後驗機率。

以下是一個使用 Python 使現文本分類的範例
```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer

# 示範文本數據
texts = ["這部電影很好看", "服務很差", "餐點美味", "環境很糟"]
labels = ["正面", "負面", "正面", "負面"]

# 將文本轉換為特徵向量
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(texts)

# 訓練模型
clf = MultinomialNB()
clf.fit(X, labels)

# 預測新文本
new_text = ["這家餐廳很棒"]
new_X = vectorizer.transform(new_text)
prediction = clf.predict(new_X)
```

### 邏輯迴歸(logistic regression, logreg)
邏輯迴歸是一種二元分類問題的基礎演算法。儘管名稱中有「迴歸」，但實際上是一個分類模型，通過 **sigmoid 函數** 將線性預測轉換成機率值。

```python
from sklearn.linear_model import LogisticRegression
import numpy as np

# 示範數據
X = np.array([[1, 2], [2, 3], [3, 4], [4, 5]])
y = np.array([0, 0, 1, 1])

# 創建和訓練模型
model = LogisticRegression()
model.fit(X, y)

# 預測
prediction = model.predict([[2.5, 3.5]])
```

## 早期的神經網路
1980年代的神經網路發展奠定了現代深度學習的基礎。反向傳播算法的發明是一個重要突破，它提供了一種有效的方法來訓練多層神經網路。  

1989年 Yann LeCun 結合卷積神經網路(Convolutional Neural Network, CNN) 和反向傳播，應用於手寫數字分類問題。

### 反向傳播(Backpropagation)
反向傳播是一種梯度下降最佳化(gradient-descent-optimization)的演算法，用於調整神經網路中的權重。其過程可以簡化為：

1. 前向傳播：輸入數據通過網路產生預測
2. 計算誤差：比較預測值與實際值
3. 反向傳播誤差：從輸出層向輸入層計算梯度
3. 更新權重：使用梯度下降來優化網路參數

以下是一個使用 PyTorch 實現簡單神經網路的例子：
```python
import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.layer1 = nn.Linear(784, 128)
        self.layer2 = nn.Linear(128, 10)
        
    def forward(self, x):
        x = torch.relu(self.layer1(x))
        x = self.layer2(x)
        return x

# 創建模型
model = SimpleNN()
```

## Kernel methods 與 SVM
支持向量機（Support Vector Methods, SVM）是一種強大的分類算法，通過在高維空間中尋找最優分類超平面來工作。

+ 目標是兩種類別的資料點間，找到最佳決策邊界(decision boundaries)
  + 步驟： 
    1. 映射到高維空間，決策邊界通常是一個超曲面
    2. 找到最大化邊界(maximizing the margin)，以製造最大的 margin
  
### Kernel tricks
將資料映射到高維空間在理論上可行，但在實際上卻常常很難處理(因為維度高，計算負載變大)  
我們不必在高維度空間把每一個資料點都做座標轉換，然後在高維度上計算決策超平面，取而代之，只要計算高維空間中點與點之間的距離即可。  
+ kernel function 定義是「初始空間中任意兩點」映射到「目標表示空間中對應點之間的距離」。
+ 白話解釋，可以將 Kernel tricks 理解為一種「偷懶」的聰明方法。想像我們要分類兩組點，在二維平面上無法用直線分開，需要將點映射到三維空間。但直接計算三維座標會很耗時，kernel tricks 讓我們可以直接計算點之間在高維空間的距離，而不需要真的去計算高維座標。
  
```python
from sklearn.svm import SVC

# 創建SVM分類器
svm = SVC(kernel='rbf')  # 使用RBF核函數
svm.fit(X_train, y_train)

# 預測
predictions = svm.predict(X_test)
```

+ 但在 SVM 中，只有決策超平面是學習而來的，kernel function 是需要透過人工設計的。SVM 在簡單分類問題上表現很好，是少數具備廣泛理論支持也經得起嚴謹數學分析的機器學習演算法。但 SVM 難以擴展到大型資料集。SVM 是一種淺層方法，需要有效的手動萃取有用的表示法，該步驟稱為**特徵工程(feature engineering)**。
+ 打個比方，圖片的原始像素無法很好的分類手寫數字，需要人工找到有用的表示法，如像素直方圖。


## 決策樹、隨機森林和梯度提升機器
### 決策樹 Decision Tree
決策樹模擬人類決策過程，通過一系列問題將數據分類。它像是一個流程圖，從根節點開始，根據特徵值選擇分支，最終到達葉節點得到預測結果。  

```python
from sklearn.tree import DecisionTreeClassifier

# 創建決策樹
tree = DecisionTreeClassifier(max_depth=3)
tree.fit(X_train, y_train)
```

### 隨機森林 Random Forest
隨機森林是多個決策樹的集成，每棵樹使用隨機選擇的特徵和數據樣本進行訓練。最終預測是所有樹的投票結果。  
```python
from sklearn.ensemble import RandomForestClassifier

# 創建隨機森林
rf = RandomForestClassifier(n_estimators=100)
rf.fit(X_train, y_train)
```

### 梯度提升機 Gradient Boosting Machines (GBM)
梯度提升機是一種迭代式的集成方法，每次迭代都試圖糾正之前模型的錯誤。  
```python
from sklearn.ensemble import GradientBoostingClassifier

# 創建梯度提升機
gbm = GradientBoostingClassifier(n_estimators=100)
gbm.fit(X_train, y_train)
```

## 深度學習最大的優勢
+ **讓特徵工程變成自動化**
+ 淺層學習有**快速遞減(fast-diminishing)** 的現象，因為「在三層模型中最佳的第一表示層，並非單層或雙層模型中最佳的第一表示層。」。
  > 想像你在蓋一棟三層樓的房子。如果你先蓋一層樓的房子，再加蓋第二層，最後加蓋第三層，這樣的結果往往不如一開始就規劃好三層樓一起設計來得好。因為當你一開始就知道要蓋三層，你可以更好地規劃整體結構，確保每一層都能最好地支持整棟建築。  
+ 聯合學習：調整模型內部一個參數，其它相關參數也會自動調整。

## 現代機器學習框架應用
### 主流框架比較

1. Scikit-learn：適用於傳統機器學習算法
  + 優點：API簡單，適合快速原型開發
  + 應用：特徵工程、傳統算法實現

2. TensorFlow/Keras：
  + 優點：生產環境部署成熟，生態系統完整
  + 應用：深度學習模型開發和部署

3. PyTorch：
  + 優點：動態計算圖，研究友好
  + 應用：研究實驗，快速迭代開發