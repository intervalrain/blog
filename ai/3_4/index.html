<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 3-4. 線性迴歸 | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="The target of machine learning"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/3_4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/3_4/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 3-4. 線性迴歸"><meta property="og:description" content="The target of machine learning"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-19T15:01:12+08:00"><meta property="article:modified_time" content="2024-12-19T15:01:12+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 3-4. 線性迴歸"><meta name=twitter:description content="The target of machine learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 3-4. 線性迴歸","item":"https://intervalrain.github.io/ai/3_4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 3-4. 線性迴歸","name":"[AI] 3-4. 線性迴歸","description":"The target of machine learning","keywords":["AI"],"articleBody":"目標 機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。 Task 代表機器學習的目標 Regression: 透過迴歸來預測值。 Classification: 處理分類問題。 Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI) Scenario 代表解決問題的策略 Supervised Learning: 使用已標記的訓練數據進行訓練 Semi-supervised Learning: 使用有標記與無標記的訓練數據進行訓練 Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構 Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。 Transfer Learning: 將一個任務學習到的知識應用到相關的新任務 Method 指應用的方法 Linear Model Deep Learning SVM Decision Tree KNN 線性迴歸 暴力解 假設我們大概知道答案的區間，我們可以暴力求解，將每一個 w, b 代入求最小的 (w, b) 組合 這個方法的缺點是，計算量很大，且我們求值的方式不是連續的，精準度不夠。 import sys areas = data[:,0] prices = data[:,1] def compute_loss(y_pred, y): return (y_pred - y)**2 best_w = 0. best_b = 0. min_loss = sys.float_info.max # 猜 w=30-50, step = 0.1 # 猜 b=200-600 step = 1 for i in range(200): for j in range(400): w = 30 + i*0.1 b = 200 + j*1 loss = 0. for area, price in zip(areas, prices): y_pred = w * area + b loss += compute_loss(y_pred, price) if loss \u003c min_loss: min_loss = loss best_w = w best_b = b w=35.1 b=599 線性代數解法 假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是： 設迴歸方程式為 $$\\text{y}=\\text{wx}+\\text{b}\\quad\\quad (1)$$\n我們要求最小平方差 $$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{y})^2\\quad\\quad (2)$$\n將 (1) 代入 (2)\n$$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{wx}-\\text{b})^2\\quad\\quad (3)$$\n學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=0$$\n$$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=0$$\n對 b 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=-2\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i-\\text{nb}=0$$\n$$\\text{n}\\bar{\\text{y}}-\\text{n}\\bar{\\text{wx}}-\\text{nb}=0$$\n$$\\text{b}=\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}}\\quad\\quad (4)$$\n對 w 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=-2\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\text{b}\\sum _{i=0}^n\\text{x}_i=0$$\n代入 (4) $$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-(\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}})\\sum _{i=0}^n\\text{x}_i=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i+\\text{w}\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}=0$$\n$$\\text{w}(\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}-\\sum _{i=0}^n\\text{x}_i^2)=\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}(\\text{n}\\bar{\\text{x}}^2-\\sum _{i=0}^n\\text{x}_i^2)=\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}=\\frac{\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}}{\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2}$$\n$$\\text{w}=\\frac{\\sum\\text{y}_i(\\text{x}_i-\\bar{\\text{x}})}{\\sum\\text{x}_i(\\text{x}_i-\\bar{\\text{x}})}$$\n$$\\text{w}=\\frac{\\sum(\\text{y}-\\bar{\\text{y}})(\\text{x}-\\bar{\\text{x}})}{\\sum(\\text{x}_i-\\bar{\\text{x}})^2}$$\n$$\\text{w}=\\frac{S_{XY}}{S_{XX}}\\quad\\quad(5)$$\n換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式 $$\\text{y}=\\frac{S_{XY}}{S_{XX}}\\text{x}+(\\bar{\\text{y}}-\\frac{S_{XY}}{S_{XX}}\\bar{\\text{x}})$$\n其中\n$$S_{XY}=\\sum(\\text{x}_i-\\bar{\\text{x}})(\\text{y}_i-\\bar{\\text{y}})=\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}$$\n$$S_{XX}=\\sum(\\text{x}_i-\\bar{\\text{x}})^2=\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2$$\n直接運用於 sample:\nimport matplotlib.pyplot as plt meanx = data[:, 0].mean() meany = data[:, 1].mean() sxy = 0.0 sxx = 0.0 for i in range(data.shape[0]): sxy += (data[i,0] - meanx)*(data[i,1] - meany) sxx += (data[i,0] - meanx)**2 w = sxy/sxx b = meany - w*meanx plt.figure(figsize=(10, 6)) plt.scatter(data[:, 0], data[:, 1], alpha=0.5) plt.plot(data[:, 0], w*data[:, 0] + b, color='red', label='Regression Line') plt.xlabel('Size (ping)') plt.ylabel('Total Price (10k)') plt.title('House Price versus House Size') plt.legend() plt.grid(True) plt.show() print(f\"w = {w:.4f}\") print(f\"b = {b:.4f}\") w = 34.9738 b = 602.5411 梯度下降(gradient descent) 但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。 再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。 import numpy as np import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt from matplotlib import cm # 1. 資料正規化函數 def normalize_data(data): return (data - np.mean(data, axis=0)) / np.std(data, axis=0) # 2. 建立並訓練模型的函數 def train_linear_regression(x_norm, y_norm, learning_rate=0.01, epochs=10): # 建立模型 model = keras.Sequential([ keras.layers.Dense(1, input_shape=(1,)) ]) # 編譯模型 optimizer = keras.optimizers.SGD(learning_rate=learning_rate) model.compile(optimizer=optimizer, loss='mse') # 用於記錄訓練過程的參數 history = {'w': [], 'b': [], 'loss': []} class ParameterHistory(keras.callbacks.Callback): def on_epoch_begin(self, epoch, logs=None): w = self.model.layers[0].get_weights()[0][0][0] b = self.model.layers[0].get_weights()[1][0] loss = self.model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) # 訓練模型 parameter_history = ParameterHistory() model.fit(x_norm, y_norm, epochs=epochs, verbose=0, callbacks=[parameter_history]) # 記錄最後一次的參數 w = model.layers[0].get_weights()[0][0][0] b = model.layers[0].get_weights()[1][0] loss = model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) return model, history # 3. 視覺化函數 def plot_training_process(x_raw, y_raw, x_norm, y_norm, history): # 創建圖表 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # 將正規化的係數轉換回原始尺度 w_raw_history = [w * np.std(y_raw) / np.std(x_raw) for w in history['w']] b_raw_history = [(b * np.std(y_raw) + np.mean(y_raw) - w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) for w, b in zip(history['w'], history['b'])] # Contour plot with raw scale margin_w = (max(w_raw_history) - min(w_raw_history)) * 0.5 margin_b = (max(b_raw_history) - min(b_raw_history)) * 0.5 w_raw_range = np.linspace(min(w_raw_history)-margin_w, max(w_raw_history)+margin_w, 100) b_raw_range = np.linspace(min(b_raw_history)-margin_b, max(b_raw_history)+margin_b, 100) W_RAW, B_RAW = np.meshgrid(w_raw_range, b_raw_range) Z = np.zeros_like(W_RAW) # 計算每個點的 MSE（在原始尺度上） for i in range(W_RAW.shape[0]): for j in range(W_RAW.shape[1]): y_pred = W_RAW[i,j] * x_raw + B_RAW[i,j] Z[i,j] = np.mean((y_pred - y_raw) ** 2) CS = ax1.contour(W_RAW, B_RAW, Z, levels=20) ax1.clabel(CS, inline=True, fontsize=8) ax1.plot(w_raw_history, b_raw_history, 'r.-', label='Training path') ax1.set_xlabel('w (原始尺度)') ax1.set_ylabel('b (原始尺度)') ax1.set_title('Contour Plot with Training Path (原始尺度)') ax1.legend() # Raw data scatter plot with regression lines ax2.scatter(x_raw, y_raw, alpha=0.5, label='Raw data') ax2.set_ylim(700, 2300) # 繪製每一輪的回歸線 x_plot = np.linspace(min(x_raw), max(x_raw), 100) colors = cm.rainbow(np.linspace(0, 1, len(w_raw_history))) for i, (w, b) in enumerate(zip(w_raw_history, b_raw_history)): y_plot = w * x_plot + b ax2.plot(x_plot, y_plot, color=colors[i], alpha=0.3) ax2.set_xlabel('Area (坪)') ax2.set_ylabel('Price (萬)') ax2.set_title('Raw Data with Regression Lines') plt.tight_layout() plt.show() # 載入數據 data = load_data() x_raw, y_raw = data[:, 0], data[:, 1] # 轉換為 TensorFlow 格式 x_raw = x_raw.reshape(-1, 1) y_raw = y_raw.reshape(-1, 1) # 正規化數據 x_norm = normalize_data(x_raw) y_norm = normalize_data(y_raw) # 訓練模型 model, history = train_linear_regression(x_norm, y_norm) # 視覺化結果 plot_training_process(x_raw.flatten(), y_raw.flatten(), x_norm.flatten(), y_norm.flatten(), history) # 輸出最終結果 final_w = history['w'][-1] final_b = history['b'][-1] final_loss = history['loss'][-1] # 將係數轉換回原始尺度 w_raw = final_w * np.std(y_raw) / np.std(x_raw) b_raw = (final_b * np.std(y_raw) + np.mean(y_raw) - final_w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) print(f\"Final equation: y = {w_raw[0]:.2f}x + {b_raw[0]:.2f}\") print(f\"Final normalized loss: {final_loss:.6f}\") 批次訓練(Batch)的概念 我們可以在不同的時機點來更新 w 與 b，假設我們的訓練次數為 3000，那 epochs 為3000。且樣本數為 1000。 批次訓練(batch training)，代表的是我們總共做 3000 次的更新，每次都是利用全部 1000 筆樣本算出來的 dw 與 db 去做調整。 SGD(Stochastic gradient descent)，代表我們做 3000 * 1000 次的更新，每一個樣本計算出 dw 與 db 就立即去調整。 小批次訓練(mini-batch training)則是介於批次訓練與 SGD 之間，假設我們每 200 個樣本做一次更新，實際上會做 3000 * 5 次更新。 import numpy as np import matplotlib.pyplot as plt class LinearRegression: def __init__(self, learning_rate=0.0000001): self.w = 0.0 self.b = 0.0 self.lr = learning_rate self.loss_history = [] def predict(self, X): return self.w * X + self.b def compute_loss(self, X, y): y_pred = self.predict(X) return np.mean((y_pred - y) ** 2) def compute_gradients(self, X, y): y_pred = self.predict(X) error = y_pred - y dw = np.mean(2 * error * X) db = np.mean(2 * error) return dw, db def train_batch(self, X, y, epochs=3000): \"\"\"Full batch gradient descent\"\"\" for epoch in range(epochs): # Compute gradients using all data dw, db = self.compute_gradients(X, y) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") def train_mini_batch(self, X, y, batch_size=2, epochs=3000): \"\"\"Mini-batch gradient descent\"\"\" n_samples = len(X) for epoch in range(epochs): # Shuffle the data indices = np.random.permutation(n_samples) X_shuffled = X[indices] y_shuffled = y[indices] # Mini-batch training for i in range(0, n_samples, batch_size): X_batch = X_shuffled[i:i+batch_size] y_batch = y_shuffled[i:i+batch_size] # Compute gradients using batch data dw, db = self.compute_gradients(X_batch, y_batch) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss for the whole dataset if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") def train_sgd(self, X, y, epochs=3000): \"\"\"Stochastic gradient descent\"\"\" n_samples = len(X) for epoch in range(epochs): # Shuffle the data indices = np.random.permutation(n_samples) X_shuffled = X[indices] y_shuffled = y[indices] # SGD training (batch_size = 1) for i in range(n_samples): X_sample = X_shuffled[i:i+1] y_sample = y_shuffled[i:i+1] # Compute gradients using single sample dw, db = self.compute_gradients(X_sample, y_sample) # Update parameters self.w -= self.lr * dw self.b -= self.lr * db # Record loss for the whole dataset if epoch % 100 == 0: loss = self.compute_loss(X, y) self.loss_history.append(loss) print(f\"Epoch {epoch}, Loss: {loss:.2f}\") (areas, prices) = load_data() models = { 'Batch': LinearRegression(learning_rate=1e-7), 'Mini-batch': LinearRegression(learning_rate=1e-7), 'SGD': LinearRegression(learning_rate=5e-8) } models['Batch'].train_batch(areas, prices) models['Mini-batch'].train_mini_batch(areas, prices) models['SGD'].train_sgd(areas, prices) plt.figure(figsize=(10, 6)) for name, model in models.items(): plt.plot(range(0, 3000, 100), model.loss_history, label=name) plt.xlabel('Epoch') plt.ylabel('Loss') plt.title('Loss Comparison') plt.legend() plt.grid(True) plt.show() for name, model in models.items(): print(f\"\\n{name} Results:\") print(f\"w = {model.w:.6f}\") print(f\"b = {model.b:.6f}\") print(f\"Final Loss = {model.loss_histroy[-1]:.2f}\") 從更新次數來看，SGD 的更新次數 \u003e 小批次訓練 \u003e 批次訓練，SGD 所耗的時間同樣也比小批次訓練與批次訓練長，但實際上 loss 收斂的情形也比較好嗎？\n從下圖比較可見，收斂情況最佳的反而是小批次訓練，我們比較三種方法，總結一下成果： 批次訓練 Batch Gradient Descent (BGD)\n每次更新使用所有數據 穩定但計算量大 容易找到局部最優解 小批次訓練 Mini-batch Gradient Descent\n每次使用一小批數據 平衡了計算效率和更新穩定性 常用於實際應用 Stochastic Gradient Descent (SGD)\n每次只使用一個樣本 更新頻繁，收斂較快但不穩定 需要較小的學習率 L1/L2 正則化(L1/L2 Regularization) 在考慮有多個特徵、且帶有 outlier 或雜訊時\nL1 正則化 (Lasso Regression)\nLasso (Least Absolute Shrinkage and Selection Operator) 定義：在損失函數中加入參數的絕對值項 $$ \\text{Loss} = \\text{MSE} + \\lambda \\times \\sum|w| $$ 特點： 傾向於產生稀疏解（某些參數會變成0） 適合用於特徵選擇 對異常值較不敏感 L2 正則化 (Ridge Regression)\n定義：在損失函數中加入參數的平方項 $$ \\text{Loss} = \\text{MSE} + \\lambda \\times \\sum(w^2) $$ 特點： 傾向於使所有參數值變小但不為0 計算導數較簡單 對共線性（多重共線性）問題有好處 import numpy as np import matplotlib.pyplot as plt from sklearn.preprocessing import StandardScaler from sklearn.model_selection import train_test_split # 生成合成數據 np.random.seed(42) def generate_synthetic_data(n_samples=100): # 生成基本特徵 X1 = np.random.normal(0, 1, n_samples) # 面積 X2 = np.random.normal(0, 1, n_samples) # 房齡 # 生成共線性特徵（與面積高度相關的特徵，如房間數） X3 = 0.8 * X1 + 0.2 * np.random.normal(0, 1, n_samples) # 生成噪音特徵（完全無關的特徵） X4 = np.random.normal(0, 1, n_samples) # 組合特徵 X = np.column_stack([X1, X2, X3, X4]) # 生成目標值（房價） # 主要由X1和X2決定，X3有少許影響，X4完全不影響 y = 3 * X1 + 2 * X2 + 0.5 * X3 + np.random.normal(0, 0.1, n_samples) return X, y class RegularizedRegression: def __init__(self, reg_type=None, lambda_reg=0.1, learning_rate=0.01): self.reg_type = reg_type self.lambda_reg = lambda_reg self.lr = learning_rate self.w = None self.loss_history = [] def fit(self, X, y, epochs=1000): n_features = X.shape[1] self.w = np.zeros(n_features) for epoch in range(epochs): # 預測 y_pred = np.dot(X, self.w) # 計算梯度 gradient = 2/len(y) * np.dot(X.T, (y_pred - y)) # 加入正則化項的梯度 if self.reg_type == 'l1': gradient += self.lambda_reg * np.sign(self.w) elif self.reg_type == 'l2': gradient += 2 * self.lambda_reg * self.w # 更新權重 self.w -= self.lr * gradient # 記錄損失 if epoch % 100 == 0: loss = np.mean((y_pred - y) ** 2) if self.reg_type == 'l1': loss += self.lambda_reg * np.sum(np.abs(self.w)) elif self.reg_type == 'l2': loss += self.lambda_reg * np.sum(self.w ** 2) self.loss_history.append(loss) def predict(self, X): return np.dot(X, self.w) # 生成數據 X, y = generate_synthetic_data(n_samples=100) # 分割訓練集和測試集 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # 標準化特徵 scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_test_scaled = scaler.transform(X_test) # 訓練不同的模型 models = { 'No Regularization': RegularizedRegression(reg_type=None, lambda_reg=0), 'L1 (Lasso)': RegularizedRegression(reg_type='l1', lambda_reg=0.1), 'L2 (Ridge)': RegularizedRegression(reg_type='l2', lambda_reg=0.1) } # 訓練模型 for name, model in models.items(): print(f\"\\nTraining {name}...\") model.fit(X_train_scaled, y_train) # 比較特徵權重 feature_names = ['Area', 'Age', 'Rooms', 'Noise'] plt.figure(figsize=(12, 6)) x = np.arange(len(feature_names)) width = 0.25 for i, (name, model) in enumerate(models.items()): plt.bar(x + i*width, model.w, width, label=name) plt.xlabel('Features') plt.ylabel('Weight') plt.title('Feature Weights Comparison') plt.xticks(x + width, feature_names) plt.legend() plt.grid(True) plt.show() # 比較測試集上的表現 plt.figure(figsize=(12, 6)) for name, model in models.items(): y_pred = model.predict(X_test_scaled) mse = np.mean((y_test - y_pred) ** 2) print(f\"\\n{name} Test MSE: {mse:.4f}\") plt.scatter(y_test, y_pred, label=f'{name} (MSE: {mse:.4f})') plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2) plt.xlabel('True Values') plt.ylabel('Predictions') plt.title('Prediction vs True Values') plt.legend() plt.grid(True) plt.show() # 印出每個特徵的權重 print(\"\\nFeature Weights:\") for name, model in models.items(): print(f\"\\n{name}:\") for fname, w in zip(feature_names, model.w): print(f\"{fname}: {w:.4f}\") 觀察重點：\nL1 正則化（Lasso）： 傾向於將不重要的特徵（如 Noise）權重設為 0 在有共線性的特徵中選擇一個（Area vs Rooms） L2 正則化（Ridge）： 所有權重都被縮小 共線性特徵的權重會被平均分配 無正則化： 可能過度擬合噪音 在共線性特徵上表現不穩定 從結果可以看出：\nL1 正則化確實將無關特徵（Noise）的權重降到接近 0 L2 正則化讓所有權重都變得更小，但保持了相對重要性 無正則化的模型權重更大，更容易受噪音影響 主要的差異和實作細節：\n正則化項的加入 L1：在損失函數中加入 λ * |w| L2：在損失函數中加入 λ * w² 梯度計算 L1 的梯度：sign(w) * λ L2 的梯度：2 * λ * w 超參數 λ (lambda_reg) 控制正則化的強度 較大的 λ 會產生較小的權重 需要通過交叉驗證來選擇適當的值 使用場景 L1：特徵選擇，當你認為只有部分特徵是重要的 L2：處理共線性，當特徵之間有相關性 ","wordCount":"1621","inLanguage":"zh-tw","datePublished":"2024-12-19T15:01:12+08:00","dateModified":"2024-12-19T15:01:12+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/3_4/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 3-4. 線性迴歸</h1><div class=post-description>The target of machine learning</div><div class=post-meta><span title='2024-12-19 15:01:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;8 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/3_4.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e7%9b%ae%e6%a8%99 aria-label=目標>目標</a></li><li><a href=#%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8 aria-label=線性迴歸>線性迴歸</a><ul><li><a href=#%e6%9a%b4%e5%8a%9b%e8%a7%a3 aria-label=暴力解>暴力解</a></li><li><a href=#%e7%b7%9a%e6%80%a7%e4%bb%a3%e6%95%b8%e8%a7%a3%e6%b3%95 aria-label=線性代數解法>線性代數解法</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgradient-descent aria-label="梯度下降(gradient descent)">梯度下降(gradient descent)</a></li><li><a href=#%e6%89%b9%e6%ac%a1%e8%a8%93%e7%b7%b4batch%e7%9a%84%e6%a6%82%e5%bf%b5 aria-label=批次訓練(Batch)的概念>批次訓練(Batch)的概念</a></li><li><a href=#l1l2-%e6%ad%a3%e5%89%87%e5%8c%96l1l2-regularization aria-label="L1/L2 正則化(L1/L2 Regularization)">L1/L2 正則化(L1/L2 Regularization)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=目標>目標<a hidden class=anchor aria-hidden=true href=#目標>#</a></h2><ul><li>機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。
<img alt=terminology loading=lazy src=/ai/AI/3_4/terminology.png><ul><li>Task 代表機器學習的目標<ul><li>Regression: 透過迴歸來預測值。</li><li>Classification: 處理分類問題。</li><li>Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI)</li></ul></li><li>Scenario 代表解決問題的策略<ul><li>Supervised Learning: 使用<strong>已標記</strong>的訓練數據進行訓練</li><li>Semi-supervised Learning: 使用<strong>有標記</strong>與<strong>無標記</strong>的訓練數據進行訓練</li><li>Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構</li><li>Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。</li><li>Transfer Learning: 將一個任務學習到的知識應用到相關的新任務</li></ul></li><li>Method 指應用的方法<ul><li>Linear Model</li><li>Deep Learning</li><li>SVM</li><li>Decision Tree</li><li>KNN</li></ul></li></ul></li></ul><h2 id=線性迴歸>線性迴歸<a hidden class=anchor aria-hidden=true href=#線性迴歸>#</a></h2><p><img alt=sample loading=lazy src=/ai/AI/3_4/sample.png></p><h3 id=暴力解>暴力解<a hidden class=anchor aria-hidden=true href=#暴力解>#</a></h3><ul><li>假設我們大概知道答案的區間，我們可以暴力求解，將每一個 w, b 代入求最小的 (w, b) 組合</li><li>這個方法的缺點是，計算量很大，且我們求值的方式不是連續的，精準度不夠。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> sys
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>areas <span style=color:#f92672>=</span> data[:,<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>prices <span style=color:#f92672>=</span> data[:,<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(y_pred, y):
</span></span><span style=display:flex><span>  <span style=color:#66d9ef>return</span> (y_pred <span style=color:#f92672>-</span> y)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>best_w <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>best_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>min_loss <span style=color:#f92672>=</span> sys<span style=color:#f92672>.</span>float_info<span style=color:#f92672>.</span>max
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 猜 w=30-50, step = 0.1</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 猜 b=200-600 step = 1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>200</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>400</span>):
</span></span><span style=display:flex><span>        w <span style=color:#f92672>=</span> <span style=color:#ae81ff>30</span> <span style=color:#f92672>+</span> i<span style=color:#f92672>*</span><span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>        b <span style=color:#f92672>=</span> <span style=color:#ae81ff>200</span> <span style=color:#f92672>+</span> j<span style=color:#f92672>*</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> area, price <span style=color:#f92672>in</span> zip(areas, prices):
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> w <span style=color:#f92672>*</span> area <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>+=</span> compute_loss(y_pred, price)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> loss <span style=color:#f92672>&lt;</span> min_loss:
</span></span><span style=display:flex><span>            min_loss <span style=color:#f92672>=</span> loss
</span></span><span style=display:flex><span>            best_w <span style=color:#f92672>=</span> w
</span></span><span style=display:flex><span>            best_b <span style=color:#f92672>=</span> b
</span></span></code></pre></div><ul><li><code>w=35.1</code></li><li><code>b=599</code>
<img alt=brute_force loading=lazy src=/ai/AI/3_4/brute_force.png></li></ul><h3 id=線性代數解法>線性代數解法<a hidden class=anchor aria-hidden=true href=#線性代數解法>#</a></h3><ul><li>假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是：<ul><li><p>設迴歸方程式為
$$\text{y}=\text{wx}+\text{b}\quad\quad (1)$$</p></li><li><p>我們要求最小平方差
$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{y})^2\quad\quad (2)$$</p></li><li><p>將 (1) 代入 (2)<br>$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{wx}-\text{b})^2\quad\quad (3)$$</p></li><li><p>學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。
$$\frac{\partial\text{L}}{\partial\text{w}}=0$$</p><p>$$\frac{\partial\text{L}}{\partial\text{b}}=0$$</p></li><li><p>對 b 做偏微分
$$\frac{\partial\text{L}}{\partial\text{b}}=-2\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i-\text{nb}=0$$</p><p>$$\text{n}\bar{\text{y}}-\text{n}\bar{\text{wx}}-\text{nb}=0$$</p><p>$$\text{b}=\bar{\text{y}}-\text{w}\bar{\text{x}}\quad\quad (4)$$</p></li><li><p>對 w 做偏微分
$$\frac{\partial\text{L}}{\partial\text{w}}=-2\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\text{b}\sum _{i=0}^n\text{x}_i=0$$</p><ul><li>代入 (4)</li></ul><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-(\bar{\text{y}}-\text{w}\bar{\text{x}})\sum _{i=0}^n\text{x}_i=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\bar{\text{y}}\sum _{i=0}^n\text{x}_i+\text{w}\sum _{i=0}^n\text{x}_i\bar{\text{x}}=0$$</p><p>$$\text{w}(\sum _{i=0}^n\text{x}_i\bar{\text{x}}-\sum _{i=0}^n\text{x}_i^2)=\bar{\text{y}}\sum _{i=0}^n\text{x}_i-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}(\text{n}\bar{\text{x}}^2-\sum _{i=0}^n\text{x}_i^2)=\text{n}\bar{\text{x}}\bar{\text{y}}-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}=\frac{\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}}{\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2}$$</p><p>$$\text{w}=\frac{\sum\text{y}_i(\text{x}_i-\bar{\text{x}})}{\sum\text{x}_i(\text{x}_i-\bar{\text{x}})}$$</p><p>$$\text{w}=\frac{\sum(\text{y}-\bar{\text{y}})(\text{x}-\bar{\text{x}})}{\sum(\text{x}_i-\bar{\text{x}})^2}$$</p><p>$$\text{w}=\frac{S_{XY}}{S_{XX}}\quad\quad(5)$$</p></li><li><p>換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式
$$\text{y}=\frac{S_{XY}}{S_{XX}}\text{x}+(\bar{\text{y}}-\frac{S_{XY}}{S_{XX}}\bar{\text{x}})$$</p><p>其中</p><p>$$S_{XY}=\sum(\text{x}_i-\bar{\text{x}})(\text{y}_i-\bar{\text{y}})=\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}$$</p><p>$$S_{XX}=\sum(\text{x}_i-\bar{\text{x}})^2=\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2$$</p></li><li><p>直接運用於 sample:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>meanx <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>meany <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(data<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>*</span>(data[i,<span style=color:#ae81ff>1</span>] <span style=color:#f92672>-</span> meany)
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> sxy<span style=color:#f92672>/</span>sxx
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> meany <span style=color:#f92672>-</span> w<span style=color:#f92672>*</span>meanx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(data[:, <span style=color:#ae81ff>0</span>], w<span style=color:#f92672>*</span>data[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> b, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Regression Line&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Size (ping)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Total Price (10k)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;House Price versus House Size&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;w = </span><span style=color:#e6db74>{</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;b = </span><span style=color:#e6db74>{</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></li></ul><img alt=sample_with_line loading=lazy src=/ai/AI/3_4/sample_with_line.png><ul><li><code>w = 34.9738</code></li><li><code>b = 602.5411</code></li></ul></li></ul><h3 id=梯度下降gradient-descent>梯度下降(gradient descent)<a hidden class=anchor aria-hidden=true href=#梯度下降gradient-descent>#</a></h3><ul><li>但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。</li><li>再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> cm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. 資料正規化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>normalize_data</span>(data):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (data <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. 建立並訓練模型的函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_linear_regression</span>(x_norm, y_norm, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 建立模型</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>        keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 編譯模型</span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>SGD(learning_rate<span style=color:#f92672>=</span>learning_rate)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span>optimizer, loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mse&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 用於記錄訓練過程的參數</span>
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;w&#39;</span>: [], <span style=color:#e6db74>&#39;b&#39;</span>: [], <span style=color:#e6db74>&#39;loss&#39;</span>: []}
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ParameterHistory</span>(keras<span style=color:#f92672>.</span>callbacks<span style=color:#f92672>.</span>Callback):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>on_epoch_begin</span>(self, epoch, logs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>            w <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            b <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>    parameter_history <span style=color:#f92672>=</span> ParameterHistory()
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(x_norm, y_norm, epochs<span style=color:#f92672>=</span>epochs, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, callbacks<span style=color:#f92672>=</span>[parameter_history])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 記錄最後一次的參數</span>
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    b <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. 視覺化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_training_process</span>(x_raw, y_raw, x_norm, y_norm, history):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 創建圖表</span>
</span></span><span style=display:flex><span>    fig, (ax1, ax2) <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 將正規化的係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>    w_raw_history <span style=color:#f92672>=</span> [w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> history[<span style=color:#e6db74>&#39;w&#39;</span>]]
</span></span><span style=display:flex><span>    b_raw_history <span style=color:#f92672>=</span> [(b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>                     w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> w, b <span style=color:#f92672>in</span> zip(history[<span style=color:#e6db74>&#39;w&#39;</span>], history[<span style=color:#e6db74>&#39;b&#39;</span>])]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Contour plot with raw scale</span>
</span></span><span style=display:flex><span>    margin_w <span style=color:#f92672>=</span> (max(w_raw_history) <span style=color:#f92672>-</span> min(w_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    margin_b <span style=color:#f92672>=</span> (max(b_raw_history) <span style=color:#f92672>-</span> min(b_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    w_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(w_raw_history)<span style=color:#f92672>-</span>margin_w, max(w_raw_history)<span style=color:#f92672>+</span>margin_w, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    b_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(b_raw_history)<span style=color:#f92672>-</span>margin_b, max(b_raw_history)<span style=color:#f92672>+</span>margin_b, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    W_RAW, B_RAW <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(w_raw_range, b_raw_range)
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(W_RAW)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 計算每個點的 MSE（在原始尺度上）</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> W_RAW[i,j] <span style=color:#f92672>*</span> x_raw <span style=color:#f92672>+</span> B_RAW[i,j]
</span></span><span style=display:flex><span>            Z[i,j] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y_raw) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    CS <span style=color:#f92672>=</span> ax1<span style=color:#f92672>.</span>contour(W_RAW, B_RAW, Z, levels<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>clabel(CS, inline<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, fontsize<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>plot(w_raw_history, b_raw_history, <span style=color:#e6db74>&#39;r.-&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training path&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;w (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;b (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Contour Plot with Training Path (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Raw data scatter plot with regression lines</span>
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>scatter(x_raw, y_raw, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylim(<span style=color:#ae81ff>700</span>, <span style=color:#ae81ff>2300</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 繪製每一輪的回歸線</span>
</span></span><span style=display:flex><span>    x_plot <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(x_raw), max(x_raw), <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    colors <span style=color:#f92672>=</span> cm<span style=color:#f92672>.</span>rainbow(np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, len(w_raw_history)))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (w, b) <span style=color:#f92672>in</span> enumerate(zip(w_raw_history, b_raw_history)):
</span></span><span style=display:flex><span>        y_plot <span style=color:#f92672>=</span> w <span style=color:#f92672>*</span> x_plot <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>        ax2<span style=color:#f92672>.</span>plot(x_plot, y_plot, color<span style=color:#f92672>=</span>colors[i], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;Area (坪)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;Price (萬)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Raw Data with Regression Lines&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 載入數據</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> load_data()
</span></span><span style=display:flex><span>x_raw, y_raw <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 轉換為 TensorFlow 格式</span>
</span></span><span style=display:flex><span>x_raw <span style=color:#f92672>=</span> x_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y_raw <span style=color:#f92672>=</span> y_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 正規化數據</span>
</span></span><span style=display:flex><span>x_norm <span style=color:#f92672>=</span> normalize_data(x_raw)
</span></span><span style=display:flex><span>y_norm <span style=color:#f92672>=</span> normalize_data(y_raw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model, history <span style=color:#f92672>=</span> train_linear_regression(x_norm, y_norm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 視覺化結果</span>
</span></span><span style=display:flex><span>plot_training_process(x_raw<span style=color:#f92672>.</span>flatten(), y_raw<span style=color:#f92672>.</span>flatten(), 
</span></span><span style=display:flex><span>                        x_norm<span style=color:#f92672>.</span>flatten(), y_norm<span style=color:#f92672>.</span>flatten(), history)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 輸出最終結果</span>
</span></span><span style=display:flex><span>final_w <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;w&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_b <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;b&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_loss <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;loss&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>w_raw <span style=color:#f92672>=</span> final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw)
</span></span><span style=display:flex><span>b_raw <span style=color:#f92672>=</span> (final_b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>        final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final equation: y = </span><span style=color:#e6db74>{</span>w_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>x + </span><span style=color:#e6db74>{</span>b_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final normalized loss: </span><span style=color:#e6db74>{</span>final_loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img alt=sample_with_gradient_descent loading=lazy src=/ai/AI/3_4/sample_with_gradient_descent.png></p><h3 id=批次訓練batch的概念>批次訓練(Batch)的概念<a hidden class=anchor aria-hidden=true href=#批次訓練batch的概念>#</a></h3><ul><li>我們可以在不同的時機點來更新 w 與 b，假設我們的訓練次數為 3000，那 epochs 為3000。且樣本數為 1000。<ul><li>批次訓練(batch training)，代表的是我們總共做 3000 次的更新，每次都是利用全部 1000 筆樣本算出來的 dw 與 db 去做調整。</li><li>SGD(Stochastic gradient descent)，代表我們做 3000 * 1000 次的更新，每一個樣本計算出 dw 與 db 就立即去調整。</li><li>小批次訓練(mini-batch training)則是介於批次訓練與 SGD 之間，假設我們每 200 個樣本做一次更新，實際上會做 3000 * 5 次更新。</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>LinearRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.0000001</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>b <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> learning_rate
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loss_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>w <span style=color:#f92672>*</span> X <span style=color:#f92672>+</span> self<span style=color:#f92672>.</span>b
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_loss</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_gradients</span>(self, X, y):
</span></span><span style=display:flex><span>        y_pred <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>predict(X)
</span></span><span style=display:flex><span>        error <span style=color:#f92672>=</span> y_pred <span style=color:#f92672>-</span> y
</span></span><span style=display:flex><span>        dw <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error <span style=color:#f92672>*</span> X)
</span></span><span style=display:flex><span>        db <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> error)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> dw, db
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_batch</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Full batch gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Compute gradients using all data</span>
</span></span><span style=display:flex><span>            dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X, y)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_mini_batch</span>(self, X, y, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Mini-batch gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Shuffle the data</span>
</span></span><span style=display:flex><span>            indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(n_samples)
</span></span><span style=display:flex><span>            X_shuffled <span style=color:#f92672>=</span> X[indices]
</span></span><span style=display:flex><span>            y_shuffled <span style=color:#f92672>=</span> y[indices]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Mini-batch training</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, n_samples, batch_size):
</span></span><span style=display:flex><span>                X_batch <span style=color:#f92672>=</span> X_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                y_batch <span style=color:#f92672>=</span> y_shuffled[i:i<span style=color:#f92672>+</span>batch_size]
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Compute gradients using batch data</span>
</span></span><span style=display:flex><span>                dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X_batch, y_batch)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss for the whole dataset</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_sgd</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>3000</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Stochastic gradient descent&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        n_samples <span style=color:#f92672>=</span> len(X)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># Shuffle the data</span>
</span></span><span style=display:flex><span>            indices <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(n_samples)
</span></span><span style=display:flex><span>            X_shuffled <span style=color:#f92672>=</span> X[indices]
</span></span><span style=display:flex><span>            y_shuffled <span style=color:#f92672>=</span> y[indices]
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># SGD training (batch_size = 1)</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n_samples):
</span></span><span style=display:flex><span>                X_sample <span style=color:#f92672>=</span> X_shuffled[i:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                y_sample <span style=color:#f92672>=</span> y_shuffled[i:i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Compute gradients using single sample</span>
</span></span><span style=display:flex><span>                dw, db <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_gradients(X_sample, y_sample)
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>                <span style=color:#75715e># Update parameters</span>
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> dw
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>b <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> db
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># Record loss for the whole dataset</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>compute_loss(X, y)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>                print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>, Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(areas, prices) <span style=color:#f92672>=</span> load_data()
</span></span><span style=display:flex><span>models <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Batch&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;Mini-batch&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-7</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;SGD&#39;</span>: LinearRegression(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>5e-8</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;Batch&#39;</span>]<span style=color:#f92672>.</span>train_batch(areas, prices)
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;Mini-batch&#39;</span>]<span style=color:#f92672>.</span>train_mini_batch(areas, prices)
</span></span><span style=display:flex><span>models[<span style=color:#e6db74>&#39;SGD&#39;</span>]<span style=color:#f92672>.</span>train_sgd(areas, prices)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>plot(range(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>3000</span>, <span style=color:#ae81ff>100</span>), model<span style=color:#f92672>.</span>loss_history, label<span style=color:#f92672>=</span>name)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Epoch&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Loss&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Loss Comparison&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74> Results:&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;w = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;b = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final Loss = </span><span style=color:#e6db74>{</span>model<span style=color:#f92672>.</span>loss_histroy[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><ul><li><p>從更新次數來看，SGD 的更新次數 > 小批次訓練 > 批次訓練，SGD 所耗的時間同樣也比小批次訓練與批次訓練長，但實際上 loss 收斂的情形也比較好嗎？</p></li><li><p>從下圖比較可見，收斂情況最佳的反而是小批次訓練，我們比較三種方法，總結一下成果：
<img alt=batch loading=lazy src=/ai/AI/3_4/batch.png></p></li><li><p>批次訓練 Batch Gradient Descent (BGD)</p><ul><li>每次更新使用所有數據</li><li>穩定但計算量大</li><li>容易找到局部最優解</li></ul></li><li><p>小批次訓練 Mini-batch Gradient Descent</p><ul><li>每次使用一小批數據</li><li>平衡了計算效率和更新穩定性</li><li>常用於實際應用</li></ul></li><li><p>Stochastic Gradient Descent (SGD)</p><ul><li>每次只使用一個樣本</li><li>更新頻繁，收斂較快但不穩定</li><li>需要較小的學習率</li></ul></li></ul><h3 id=l1l2-正則化l1l2-regularization>L1/L2 正則化(L1/L2 Regularization)<a hidden class=anchor aria-hidden=true href=#l1l2-正則化l1l2-regularization>#</a></h3><ul><li><p>在考慮有多個特徵、且帶有 outlier 或雜訊時</p></li><li><p>L1 正則化 (Lasso Regression)</p><ul><li>Lasso (Least Absolute Shrinkage and Selection Operator)</li><li>定義：在損失函數中加入參數的絕對值項
$$
\text{Loss} = \text{MSE} + \lambda \times \sum|w|
$$</li><li>特點：<ul><li>傾向於產生稀疏解（某些參數會變成0）</li><li>適合用於特徵選擇</li><li>對異常值較不敏感</li></ul></li></ul></li><li><p>L2 正則化 (Ridge Regression)</p><ul><li>定義：在損失函數中加入參數的平方項
$$
\text{Loss} = \text{MSE} + \lambda \times \sum(w^2)
$$</li><li>特點：<ul><li>傾向於使所有參數值變小但不為0</li><li>計算導數較簡單</li><li>對共線性（多重共線性）問題有好處</li></ul></li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.preprocessing <span style=color:#f92672>import</span> StandardScaler
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成合成數據</span>
</span></span><span style=display:flex><span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>seed(<span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_synthetic_data</span>(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成基本特徵</span>
</span></span><span style=display:flex><span>    X1 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)  <span style=color:#75715e># 面積</span>
</span></span><span style=display:flex><span>    X2 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)  <span style=color:#75715e># 房齡</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成共線性特徵（與面積高度相關的特徵，如房間數）</span>
</span></span><span style=display:flex><span>    X3 <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.8</span> <span style=color:#f92672>*</span> X1 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.2</span> <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成噪音特徵（完全無關的特徵）</span>
</span></span><span style=display:flex><span>    X4 <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 組合特徵</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>column_stack([X1, X2, X3, X4])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 生成目標值（房價）</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 主要由X1和X2決定，X3有少許影響，X4完全不影響</span>
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span> <span style=color:#f92672>*</span> X1 <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> X2 <span style=color:#f92672>+</span> <span style=color:#ae81ff>0.5</span> <span style=color:#f92672>*</span> X3 <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.1</span>, n_samples)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X, y
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>RegularizedRegression</span>:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, reg_type<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, lambda_reg<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>):
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>=</span> reg_type
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>=</span> lambda_reg
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>lr <span style=color:#f92672>=</span> learning_rate
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> <span style=color:#66d9ef>None</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>loss_history <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit</span>(self, X, y, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>):
</span></span><span style=display:flex><span>        n_features <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>w <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n_features)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(epochs):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 預測</span>
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(X, self<span style=color:#f92672>.</span>w)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># 計算梯度</span>
</span></span><span style=display:flex><span>            gradient <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>/</span>len(y) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>dot(X<span style=color:#f92672>.</span>T, (y_pred <span style=color:#f92672>-</span> y))
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># 加入正則化項的梯度</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l1&#39;</span>:
</span></span><span style=display:flex><span>                gradient <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sign(self<span style=color:#f92672>.</span>w)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l2&#39;</span>:
</span></span><span style=display:flex><span>                gradient <span style=color:#f92672>+=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> self<span style=color:#f92672>.</span>w
</span></span><span style=display:flex><span>                
</span></span><span style=display:flex><span>            <span style=color:#75715e># 更新權重</span>
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>w <span style=color:#f92672>-=</span> self<span style=color:#f92672>.</span>lr <span style=color:#f92672>*</span> gradient
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>            <span style=color:#75715e># 記錄損失</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> epoch <span style=color:#f92672>%</span> <span style=color:#ae81ff>100</span> <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l1&#39;</span>:
</span></span><span style=display:flex><span>                    loss <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sum(np<span style=color:#f92672>.</span>abs(self<span style=color:#f92672>.</span>w))
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>elif</span> self<span style=color:#f92672>.</span>reg_type <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;l2&#39;</span>:
</span></span><span style=display:flex><span>                    loss <span style=color:#f92672>+=</span> self<span style=color:#f92672>.</span>lambda_reg <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>sum(self<span style=color:#f92672>.</span>w <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>                self<span style=color:#f92672>.</span>loss_history<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(self, X):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>dot(X, self<span style=color:#f92672>.</span>w)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 生成數據</span>
</span></span><span style=display:flex><span>X, y <span style=color:#f92672>=</span> generate_synthetic_data(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 分割訓練集和測試集</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(X, y, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 標準化特徵</span>
</span></span><span style=display:flex><span>scaler <span style=color:#f92672>=</span> StandardScaler()
</span></span><span style=display:flex><span>X_train_scaled <span style=color:#f92672>=</span> scaler<span style=color:#f92672>.</span>fit_transform(X_train)
</span></span><span style=display:flex><span>X_test_scaled <span style=color:#f92672>=</span> scaler<span style=color:#f92672>.</span>transform(X_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練不同的模型</span>
</span></span><span style=display:flex><span>models <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;No Regularization&#39;</span>: RegularizedRegression(reg_type<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, lambda_reg<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;L1 (Lasso)&#39;</span>: RegularizedRegression(reg_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;l1&#39;</span>, lambda_reg<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#39;L2 (Ridge)&#39;</span>: RegularizedRegression(reg_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;l2&#39;</span>, lambda_reg<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Training </span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>...&#34;</span>)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(X_train_scaled, y_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 比較特徵權重</span>
</span></span><span style=display:flex><span>feature_names <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;Area&#39;</span>, <span style=color:#e6db74>&#39;Age&#39;</span>, <span style=color:#e6db74>&#39;Rooms&#39;</span>, <span style=color:#e6db74>&#39;Noise&#39;</span>]
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(len(feature_names))
</span></span><span style=display:flex><span>width <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.25</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i, (name, model) <span style=color:#f92672>in</span> enumerate(models<span style=color:#f92672>.</span>items()):
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>bar(x <span style=color:#f92672>+</span> i<span style=color:#f92672>*</span>width, model<span style=color:#f92672>.</span>w, width, label<span style=color:#f92672>=</span>name)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Features&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Weight&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Feature Weights Comparison&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xticks(x <span style=color:#f92672>+</span> width, feature_names)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 比較測試集上的表現</span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>12</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    y_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(X_test_scaled)
</span></span><span style=display:flex><span>    mse <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_test <span style=color:#f92672>-</span> y_pred) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74> Test MSE: </span><span style=color:#e6db74>{</span>mse<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>scatter(y_test, y_pred, label<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74> (MSE: </span><span style=color:#e6db74>{</span>mse<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>)&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot([y_test<span style=color:#f92672>.</span>min(), y_test<span style=color:#f92672>.</span>max()], [y_test<span style=color:#f92672>.</span>min(), y_test<span style=color:#f92672>.</span>max()], <span style=color:#e6db74>&#39;k--&#39;</span>, lw<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;True Values&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Predictions&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Prediction vs True Values&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 印出每個特徵的權重</span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Feature Weights:&#34;</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> name, model <span style=color:#f92672>in</span> models<span style=color:#f92672>.</span>items():
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>{</span>name<span style=color:#e6db74>}</span><span style=color:#e6db74>:&#34;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> fname, w <span style=color:#f92672>in</span> zip(feature_names, model<span style=color:#f92672>.</span>w):
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;</span><span style=color:#e6db74>{</span>fname<span style=color:#e6db74>}</span><span style=color:#e6db74>: </span><span style=color:#e6db74>{</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img alt=regularization loading=lazy src=/ai/AI/3_4/relugarization.png></p><ul><li><p>觀察重點：</p><ul><li>L1 正則化（Lasso）：<ul><li>傾向於將不重要的特徵（如 Noise）權重設為 0</li><li>在有共線性的特徵中選擇一個（Area vs Rooms）</li></ul></li><li>L2 正則化（Ridge）：<ul><li>所有權重都被縮小</li><li>共線性特徵的權重會被平均分配</li></ul></li><li>無正則化：<ul><li>可能過度擬合噪音</li><li>在共線性特徵上表現不穩定</li></ul></li></ul></li><li><p>從結果可以看出：</p><ul><li>L1 正則化確實將無關特徵（Noise）的權重降到接近 0</li><li>L2 正則化讓所有權重都變得更小，但保持了相對重要性</li><li>無正則化的模型權重更大，更容易受噪音影響</li></ul></li><li><p>主要的差異和實作細節：</p><ol><li><strong>正則化項的加入</strong></li></ol><ul><li>L1：在損失函數中加入 <code>λ * |w|</code></li><li>L2：在損失函數中加入 <code>λ * w²</code></li></ul><ol start=2><li><strong>梯度計算</strong></li></ol><ul><li>L1 的梯度：<code>sign(w) * λ</code></li><li>L2 的梯度：<code>2 * λ * w</code></li></ul><ol start=3><li><strong>超參數 λ (lambda_reg)</strong></li></ol><ul><li>控制正則化的強度</li><li>較大的 λ 會產生較小的權重</li><li>需要通過交叉驗證來選擇適當的值</li></ul><ol start=4><li><strong>使用場景</strong></li></ol><ul><li>L1：特徵選擇，當你認為只有部分特徵是重要的</li><li>L2：處理共線性，當特徵之間有相關性</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/3_3/><span class=title>« 上一頁</span><br><span>[AI] 3-3. 使用 TensorFlow 與 Keras 函式庫</span>
</a><a class=next href=https://intervalrain.github.io/ai/3_5/><span class=title>下一頁 »</span><br><span>[AI] 3-5. 實作線性分類器</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>