<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 3-4. 線性迴歸 | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="The target of machine learning"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/3_4/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src=https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/3_4/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 3-4. 線性迴歸"><meta property="og:description" content="The target of machine learning"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2024-12-19T15:01:12+08:00"><meta property="article:modified_time" content="2024-12-19T15:01:12+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 3-4. 線性迴歸"><meta name=twitter:description content="The target of machine learning"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 3-4. 線性迴歸","item":"https://intervalrain.github.io/ai/3_4/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 3-4. 線性迴歸","name":"[AI] 3-4. 線性迴歸","description":"The target of machine learning","keywords":["AI"],"articleBody":"目標 機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。 Task 代表機器學習的目標 Regression: 透過迴歸來預測值。 Classification: 處理分類問題。 Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI) Scenario 代表解決問題的策略 Supervised Learning: 使用已標記的訓練數據進行訓練 Semi-supervised Learning: 使用有標記與無標記的訓練數據進行訓練 Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構 Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。 Transfer Learning: 將一個任務學習到的知識應用到相關的新任務 Method 指應用的方法 Linear Model Deep Learning SVM Decision Tree KNN 線性迴歸 線性代數解法 假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是： 設迴歸方程式為 $$\\text{y}=\\text{wx}+\\text{b}\\quad\\quad (1)$$\n我們要求最小平方差 $$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{y})^2\\quad\\quad (2)$$\n將 (1) 代入 (2)\n$$\\text{L}=\\sum_{i=0}^n(\\text{y}_i-\\text{wx}-\\text{b})^2\\quad\\quad (3)$$\n學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=0$$\n$$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=0$$\n對 b 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{b}}=-2\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i-\\text{nb}=0$$\n$$\\text{n}\\bar{\\text{y}}-\\text{n}\\bar{\\text{wx}}-\\text{nb}=0$$\n$$\\text{b}=\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}}\\quad\\quad (4)$$\n對 w 做偏微分 $$\\frac{\\partial\\text{L}}{\\partial\\text{w}}=-2\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i(\\text{y}_i-\\text{wx}_i-\\text{b})=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\text{b}\\sum _{i=0}^n\\text{x}_i=0$$\n代入 (4) $$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-(\\bar{\\text{y}}-\\text{w}\\bar{\\text{x}})\\sum _{i=0}^n\\text{x}_i=0$$\n$$\\sum_{i=0}^n\\text{x}_i\\text{y}_i-\\text{w}\\sum _{i=0}^n\\text{x}_i^2-\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i+\\text{w}\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}=0$$\n$$\\text{w}(\\sum _{i=0}^n\\text{x}_i\\bar{\\text{x}}-\\sum _{i=0}^n\\text{x}_i^2)=\\bar{\\text{y}}\\sum _{i=0}^n\\text{x}_i-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}(\\text{n}\\bar{\\text{x}}^2-\\sum _{i=0}^n\\text{x}_i^2)=\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}-\\sum _{i=0}^n\\text{x}_i\\text{y}_i$$\n$$\\text{w}=\\frac{\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}}{\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2}$$\n$$\\text{w}=\\frac{\\sum\\text{y}_i(\\text{x}_i-\\bar{\\text{x}})}{\\sum\\text{x}_i(\\text{x}_i-\\bar{\\text{x}})}$$\n$$\\text{w}=\\frac{\\sum(\\text{y}-\\bar{\\text{y}})(\\text{x}-\\bar{\\text{x}})}{\\sum(\\text{x}_i-\\bar{\\text{x}})^2}$$\n$$\\text{w}=\\frac{S_{XY}}{S_{XX}}\\quad\\quad(5)$$\n換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式 $$\\text{y}=\\frac{S_{XY}}{S_{XX}}\\text{x}+(\\bar{\\text{y}}-\\frac{S_{XY}}{S_{XX}}\\bar{\\text{x}})$$\n其中\n$$S_{XY}=\\sum(\\text{x}_i-\\bar{\\text{x}})(\\text{y}_i-\\bar{\\text{y}})=\\sum\\text{x}_i\\text{y}_i-\\text{n}\\bar{\\text{x}}\\bar{\\text{y}}$$\n$$S_{XX}=\\sum(\\text{x}_i-\\bar{\\text{x}})^2=\\sum\\text{x}_i^2-\\text{n}\\bar{\\text{x}}^2$$\n直接運用於 sample:\nimport matplotlib.pyplot as plt meanx = data[:, 0].mean() meany = data[:, 1].mean() sxy = 0.0 sxx = 0.0 for i in range(data.shape[0]): sxy += (data[i,0] - meanx)*(data[i,1] - meany) sxx += (data[i,0] - meanx)**2 w = sxy/sxx b = meany - w*meanx plt.figure(figsize=(10, 6)) plt.scatter(data[:, 0], data[:, 1], alpha=0.5) plt.plot(data[:, 0], w*data[:, 0] + b, color='red', label='Regression Line') plt.xlabel('Size (ping)') plt.ylabel('Total Price (10k)') plt.title('House Price versus House Size') plt.legend() plt.grid(True) plt.show() print(f\"w = {w:.4f}\") print(f\"b = {b:.4f}\") w = 34.9738 b = 602.5411 梯度下降(gradient descent) 但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。 再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。 import numpy as np import tensorflow as tf from tensorflow import keras import matplotlib.pyplot as plt from matplotlib import cm # 1. 資料正規化函數 def normalize_data(data): return (data - np.mean(data, axis=0)) / np.std(data, axis=0) # 2. 建立並訓練模型的函數 def train_linear_regression(x_norm, y_norm, learning_rate=0.01, epochs=10): # 建立模型 model = keras.Sequential([ keras.layers.Dense(1, input_shape=(1,)) ]) # 編譯模型 optimizer = keras.optimizers.SGD(learning_rate=learning_rate) model.compile(optimizer=optimizer, loss='mse') # 用於記錄訓練過程的參數 history = {'w': [], 'b': [], 'loss': []} class ParameterHistory(keras.callbacks.Callback): def on_epoch_begin(self, epoch, logs=None): w = self.model.layers[0].get_weights()[0][0][0] b = self.model.layers[0].get_weights()[1][0] loss = self.model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) # 訓練模型 parameter_history = ParameterHistory() model.fit(x_norm, y_norm, epochs=epochs, verbose=0, callbacks=[parameter_history]) # 記錄最後一次的參數 w = model.layers[0].get_weights()[0][0][0] b = model.layers[0].get_weights()[1][0] loss = model.evaluate(x_norm, y_norm, verbose=0) history['w'].append(w) history['b'].append(b) history['loss'].append(loss) return model, history # 3. 視覺化函數 def plot_training_process(x_raw, y_raw, x_norm, y_norm, history): # 創建圖表 fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6)) # 將正規化的係數轉換回原始尺度 w_raw_history = [w * np.std(y_raw) / np.std(x_raw) for w in history['w']] b_raw_history = [(b * np.std(y_raw) + np.mean(y_raw) - w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) for w, b in zip(history['w'], history['b'])] # Contour plot with raw scale margin_w = (max(w_raw_history) - min(w_raw_history)) * 0.5 margin_b = (max(b_raw_history) - min(b_raw_history)) * 0.5 w_raw_range = np.linspace(min(w_raw_history)-margin_w, max(w_raw_history)+margin_w, 100) b_raw_range = np.linspace(min(b_raw_history)-margin_b, max(b_raw_history)+margin_b, 100) W_RAW, B_RAW = np.meshgrid(w_raw_range, b_raw_range) Z = np.zeros_like(W_RAW) # 計算每個點的 MSE（在原始尺度上） for i in range(W_RAW.shape[0]): for j in range(W_RAW.shape[1]): y_pred = W_RAW[i,j] * x_raw + B_RAW[i,j] Z[i,j] = np.mean((y_pred - y_raw) ** 2) CS = ax1.contour(W_RAW, B_RAW, Z, levels=20) ax1.clabel(CS, inline=True, fontsize=8) ax1.plot(w_raw_history, b_raw_history, 'r.-', label='Training path') ax1.set_xlabel('w (原始尺度)') ax1.set_ylabel('b (原始尺度)') ax1.set_title('Contour Plot with Training Path (原始尺度)') ax1.legend() # Raw data scatter plot with regression lines ax2.scatter(x_raw, y_raw, alpha=0.5, label='Raw data') ax2.set_ylim(700, 2300) # 繪製每一輪的回歸線 x_plot = np.linspace(min(x_raw), max(x_raw), 100) colors = cm.rainbow(np.linspace(0, 1, len(w_raw_history))) for i, (w, b) in enumerate(zip(w_raw_history, b_raw_history)): y_plot = w * x_plot + b ax2.plot(x_plot, y_plot, color=colors[i], alpha=0.3) ax2.set_xlabel('Area (坪)') ax2.set_ylabel('Price (萬)') ax2.set_title('Raw Data with Regression Lines') plt.tight_layout() plt.show() # 載入數據 data = load_data() x_raw, y_raw = data[:, 0], data[:, 1] # 轉換為 TensorFlow 格式 x_raw = x_raw.reshape(-1, 1) y_raw = y_raw.reshape(-1, 1) # 正規化數據 x_norm = normalize_data(x_raw) y_norm = normalize_data(y_raw) # 訓練模型 model, history = train_linear_regression(x_norm, y_norm) # 視覺化結果 plot_training_process(x_raw.flatten(), y_raw.flatten(), x_norm.flatten(), y_norm.flatten(), history) # 輸出最終結果 final_w = history['w'][-1] final_b = history['b'][-1] final_loss = history['loss'][-1] # 將係數轉換回原始尺度 w_raw = final_w * np.std(y_raw) / np.std(x_raw) b_raw = (final_b * np.std(y_raw) + np.mean(y_raw) - final_w * np.std(y_raw) * np.mean(x_raw) / np.std(x_raw)) print(f\"Final equation: y = {w_raw[0]:.2f}x + {b_raw[0]:.2f}\") print(f\"Final normalized loss: {final_loss:.6f}\") ","wordCount":"610","inLanguage":"zh-tw","datePublished":"2024-12-19T15:01:12+08:00","dateModified":"2024-12-19T15:01:12+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/3_4/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 3-4. 線性迴歸</h1><div class=post-description>The target of machine learning</div><div class=post-meta><span title='2024-12-19 15:01:12 +0800 +0800'>December 19, 2024</span>&nbsp;·&nbsp;3 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/3_4.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e7%9b%ae%e6%a8%99 aria-label=目標>目標</a></li><li><a href=#%e7%b7%9a%e6%80%a7%e8%bf%b4%e6%ad%b8 aria-label=線性迴歸>線性迴歸</a><ul><li><a href=#%e7%b7%9a%e6%80%a7%e4%bb%a3%e6%95%b8%e8%a7%a3%e6%b3%95 aria-label=線性代數解法>線性代數解法</a></li><li><a href=#%e6%a2%af%e5%ba%a6%e4%b8%8b%e9%99%8dgradient-descent aria-label="梯度下降(gradient descent)">梯度下降(gradient descent)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><h2 id=目標>目標<a hidden class=anchor aria-hidden=true href=#目標>#</a></h2><ul><li>機器學習的目標有很多種，參考李宏毅教授的機器學習課程，可以用下面一張圖來概述。
<img alt=terminology loading=lazy src=/ai/AI/3_4/terminology.png><ul><li>Task 代表機器學習的目標<ul><li>Regression: 透過迴歸來預測值。</li><li>Classification: 處理分類問題。</li><li>Structed Learning: 生成結構化的資訊(現在稱為生成式 AI, GenAI)</li></ul></li><li>Scenario 代表解決問題的策略<ul><li>Supervised Learning: 使用<strong>已標記</strong>的訓練數據進行訓練</li><li>Semi-supervised Learning: 使用<strong>有標記</strong>與<strong>無標記</strong>的訓練數據進行訓練</li><li>Unsupervised Learning: 不使用標記的訓練數據進行訓據，由模型自行發現模式與結構</li><li>Reinforcement Learning: 透過「獎勵」與「懲罰」來學習。</li><li>Transfer Learning: 將一個任務學習到的知識應用到相關的新任務</li></ul></li><li>Method 指應用的方法<ul><li>Linear Model</li><li>Deep Learning</li><li>SVM</li><li>Decision Tree</li><li>KNN</li></ul></li></ul></li></ul><h2 id=線性迴歸>線性迴歸<a hidden class=anchor aria-hidden=true href=#線性迴歸>#</a></h2><h3 id=線性代數解法>線性代數解法<a hidden class=anchor aria-hidden=true href=#線性代數解法>#</a></h3><p><img alt=sample loading=lazy src=/ai/AI/3_4/sample.png></p><ul><li>假如我們學過線性代數，我們想得到它的歸性迴歸方程式，我們的作法會是：<ul><li><p>設迴歸方程式為
$$\text{y}=\text{wx}+\text{b}\quad\quad (1)$$</p></li><li><p>我們要求最小平方差
$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{y})^2\quad\quad (2)$$</p></li><li><p>將 (1) 代入 (2)<br>$$\text{L}=\sum_{i=0}^n(\text{y}_i-\text{wx}-\text{b})^2\quad\quad (3)$$</p></li><li><p>學過線性代數，我們知道要求極值，可以對其求導數為0，並設 w 與 b 互不為函數，故我們對其個別做偏微分等於0。
$$\frac{\partial\text{L}}{\partial\text{w}}=0$$</p><p>$$\frac{\partial\text{L}}{\partial\text{b}}=0$$</p></li><li><p>對 b 做偏微分
$$\frac{\partial\text{L}}{\partial\text{b}}=-2\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i-\text{nb}=0$$</p><p>$$\text{n}\bar{\text{y}}-\text{n}\bar{\text{wx}}-\text{nb}=0$$</p><p>$$\text{b}=\bar{\text{y}}-\text{w}\bar{\text{x}}\quad\quad (4)$$</p></li><li><p>對 w 做偏微分
$$\frac{\partial\text{L}}{\partial\text{w}}=-2\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i(\text{y}_i-\text{wx}_i-\text{b})=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\text{b}\sum _{i=0}^n\text{x}_i=0$$</p><ul><li>代入 (4)</li></ul><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-(\bar{\text{y}}-\text{w}\bar{\text{x}})\sum _{i=0}^n\text{x}_i=0$$</p><p>$$\sum_{i=0}^n\text{x}_i\text{y}_i-\text{w}\sum _{i=0}^n\text{x}_i^2-\bar{\text{y}}\sum _{i=0}^n\text{x}_i+\text{w}\sum _{i=0}^n\text{x}_i\bar{\text{x}}=0$$</p><p>$$\text{w}(\sum _{i=0}^n\text{x}_i\bar{\text{x}}-\sum _{i=0}^n\text{x}_i^2)=\bar{\text{y}}\sum _{i=0}^n\text{x}_i-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}(\text{n}\bar{\text{x}}^2-\sum _{i=0}^n\text{x}_i^2)=\text{n}\bar{\text{x}}\bar{\text{y}}-\sum _{i=0}^n\text{x}_i\text{y}_i$$</p><p>$$\text{w}=\frac{\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}}{\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2}$$</p><p>$$\text{w}=\frac{\sum\text{y}_i(\text{x}_i-\bar{\text{x}})}{\sum\text{x}_i(\text{x}_i-\bar{\text{x}})}$$</p><p>$$\text{w}=\frac{\sum(\text{y}-\bar{\text{y}})(\text{x}-\bar{\text{x}})}{\sum(\text{x}_i-\bar{\text{x}})^2}$$</p><p>$$\text{w}=\frac{S_{XY}}{S_{XX}}\quad\quad(5)$$</p></li><li><p>換言之，我們可以透過 (4) 與 (5) 式直接求得迴歸方程式
$$\text{y}=\frac{S_{XY}}{S_{XX}}\text{x}+(\bar{\text{y}}-\frac{S_{XY}}{S_{XX}}\bar{\text{x}})$$</p><p>其中</p><p>$$S_{XY}=\sum(\text{x}_i-\bar{\text{x}})(\text{y}_i-\bar{\text{y}})=\sum\text{x}_i\text{y}_i-\text{n}\bar{\text{x}}\bar{\text{y}}$$</p><p>$$S_{XX}=\sum(\text{x}_i-\bar{\text{x}})^2=\sum\text{x}_i^2-\text{n}\bar{\text{x}}^2$$</p></li><li><p>直接運用於 sample:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>meanx <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>meany <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>mean()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(data<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>sxy <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>*</span>(data[i,<span style=color:#ae81ff>1</span>] <span style=color:#f92672>-</span> meany)
</span></span><span style=display:flex><span>sxx <span style=color:#f92672>+=</span> (data[i,<span style=color:#ae81ff>0</span>] <span style=color:#f92672>-</span> meanx)<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>w <span style=color:#f92672>=</span> sxy<span style=color:#f92672>/</span>sxx
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> meany <span style=color:#f92672>-</span> w<span style=color:#f92672>*</span>meanx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>figure(figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>10</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(data[:, <span style=color:#ae81ff>0</span>], w<span style=color:#f92672>*</span>data[:, <span style=color:#ae81ff>0</span>] <span style=color:#f92672>+</span> b, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;red&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Regression Line&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;Size (ping)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Total Price (10k)&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;House Price versus House Size&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>grid(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;w = </span><span style=color:#e6db74>{</span>w<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;b = </span><span style=color:#e6db74>{</span>b<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></li></ul><img alt=sample_with_line loading=lazy src=/ai/AI/3_4/sample_with_line.png><ul><li><code>w = 34.9738</code></li><li><code>b = 602.5411</code></li></ul></li></ul><h3 id=梯度下降gradient-descent>梯度下降(gradient descent)<a hidden class=anchor aria-hidden=true href=#梯度下降gradient-descent>#</a></h3><ul><li>但事實上，在機器學習的領域要處理的不一定是上述這種只有兩維的問題，多維的問題會有多個梯度為0的地方，代表我們需要求出全部梯度為0的地方，再逐一代入我們的 loss function，最後找出 loss 最小的一組答案。</li><li>再者是，加入 activation function 後的方程式，變得並非上述案例中的容易微分。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> matplotlib <span style=color:#f92672>import</span> cm
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 1. 資料正規化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>normalize_data</span>(data):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (data <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>mean(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(data, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 2. 建立並訓練模型的函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_linear_regression</span>(x_norm, y_norm, learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 建立模型</span>
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>        keras<span style=color:#f92672>.</span>layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>1</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>1</span>,))
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 編譯模型</span>
</span></span><span style=display:flex><span>    optimizer <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>SGD(learning_rate<span style=color:#f92672>=</span>learning_rate)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span>optimizer, loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;mse&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 用於記錄訓練過程的參數</span>
</span></span><span style=display:flex><span>    history <span style=color:#f92672>=</span> {<span style=color:#e6db74>&#39;w&#39;</span>: [], <span style=color:#e6db74>&#39;b&#39;</span>: [], <span style=color:#e6db74>&#39;loss&#39;</span>: []}
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>class</span> <span style=color:#a6e22e>ParameterHistory</span>(keras<span style=color:#f92672>.</span>callbacks<span style=color:#f92672>.</span>Callback):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>on_epoch_begin</span>(self, epoch, logs<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>            w <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            b <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>            loss <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>            history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>    parameter_history <span style=color:#f92672>=</span> ParameterHistory()
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(x_norm, y_norm, epochs<span style=color:#f92672>=</span>epochs, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>, callbacks<span style=color:#f92672>=</span>[parameter_history])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 記錄最後一次的參數</span>
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    b <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>layers[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>get_weights()[<span style=color:#ae81ff>1</span>][<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    loss <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(x_norm, y_norm, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;w&#39;</span>]<span style=color:#f92672>.</span>append(w)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;b&#39;</span>]<span style=color:#f92672>.</span>append(b)
</span></span><span style=display:flex><span>    history[<span style=color:#e6db74>&#39;loss&#39;</span>]<span style=color:#f92672>.</span>append(loss)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model, history
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 3. 視覺化函數</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>plot_training_process</span>(x_raw, y_raw, x_norm, y_norm, history):
</span></span><span style=display:flex><span>    <span style=color:#75715e># 創建圖表</span>
</span></span><span style=display:flex><span>    fig, (ax1, ax2) <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>, figsize<span style=color:#f92672>=</span>(<span style=color:#ae81ff>15</span>, <span style=color:#ae81ff>6</span>))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 將正規化的係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>    w_raw_history <span style=color:#f92672>=</span> [w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw) <span style=color:#66d9ef>for</span> w <span style=color:#f92672>in</span> history[<span style=color:#e6db74>&#39;w&#39;</span>]]
</span></span><span style=display:flex><span>    b_raw_history <span style=color:#f92672>=</span> [(b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>                     w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>                     <span style=color:#66d9ef>for</span> w, b <span style=color:#f92672>in</span> zip(history[<span style=color:#e6db74>&#39;w&#39;</span>], history[<span style=color:#e6db74>&#39;b&#39;</span>])]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Contour plot with raw scale</span>
</span></span><span style=display:flex><span>    margin_w <span style=color:#f92672>=</span> (max(w_raw_history) <span style=color:#f92672>-</span> min(w_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    margin_b <span style=color:#f92672>=</span> (max(b_raw_history) <span style=color:#f92672>-</span> min(b_raw_history)) <span style=color:#f92672>*</span> <span style=color:#ae81ff>0.5</span>
</span></span><span style=display:flex><span>    w_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(w_raw_history)<span style=color:#f92672>-</span>margin_w, max(w_raw_history)<span style=color:#f92672>+</span>margin_w, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    b_raw_range <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(b_raw_history)<span style=color:#f92672>-</span>margin_b, max(b_raw_history)<span style=color:#f92672>+</span>margin_b, <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    W_RAW, B_RAW <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>meshgrid(w_raw_range, b_raw_range)
</span></span><span style=display:flex><span>    Z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(W_RAW)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 計算每個點的 MSE（在原始尺度上）</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(W_RAW<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]):
</span></span><span style=display:flex><span>            y_pred <span style=color:#f92672>=</span> W_RAW[i,j] <span style=color:#f92672>*</span> x_raw <span style=color:#f92672>+</span> B_RAW[i,j]
</span></span><span style=display:flex><span>            Z[i,j] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean((y_pred <span style=color:#f92672>-</span> y_raw) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    CS <span style=color:#f92672>=</span> ax1<span style=color:#f92672>.</span>contour(W_RAW, B_RAW, Z, levels<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>clabel(CS, inline<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, fontsize<span style=color:#f92672>=</span><span style=color:#ae81ff>8</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>plot(w_raw_history, b_raw_history, <span style=color:#e6db74>&#39;r.-&#39;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Training path&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;w (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;b (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Contour Plot with Training Path (原始尺度)&#39;</span>)
</span></span><span style=display:flex><span>    ax1<span style=color:#f92672>.</span>legend()
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># Raw data scatter plot with regression lines</span>
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>scatter(x_raw, y_raw, alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.5</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;Raw data&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylim(<span style=color:#ae81ff>700</span>, <span style=color:#ae81ff>2300</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># 繪製每一輪的回歸線</span>
</span></span><span style=display:flex><span>    x_plot <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linspace(min(x_raw), max(x_raw), <span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>    colors <span style=color:#f92672>=</span> cm<span style=color:#f92672>.</span>rainbow(np<span style=color:#f92672>.</span>linspace(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, len(w_raw_history)))
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (w, b) <span style=color:#f92672>in</span> enumerate(zip(w_raw_history, b_raw_history)):
</span></span><span style=display:flex><span>        y_plot <span style=color:#f92672>=</span> w <span style=color:#f92672>*</span> x_plot <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>        ax2<span style=color:#f92672>.</span>plot(x_plot, y_plot, color<span style=color:#f92672>=</span>colors[i], alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_xlabel(<span style=color:#e6db74>&#39;Area (坪)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_ylabel(<span style=color:#e6db74>&#39;Price (萬)&#39;</span>)
</span></span><span style=display:flex><span>    ax2<span style=color:#f92672>.</span>set_title(<span style=color:#e6db74>&#39;Raw Data with Regression Lines&#39;</span>)
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>tight_layout()
</span></span><span style=display:flex><span>    plt<span style=color:#f92672>.</span>show()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 載入數據</span>
</span></span><span style=display:flex><span>data <span style=color:#f92672>=</span> load_data()
</span></span><span style=display:flex><span>x_raw, y_raw <span style=color:#f92672>=</span> data[:, <span style=color:#ae81ff>0</span>], data[:, <span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 轉換為 TensorFlow 格式</span>
</span></span><span style=display:flex><span>x_raw <span style=color:#f92672>=</span> x_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>y_raw <span style=color:#f92672>=</span> y_raw<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 正規化數據</span>
</span></span><span style=display:flex><span>x_norm <span style=color:#f92672>=</span> normalize_data(x_raw)
</span></span><span style=display:flex><span>y_norm <span style=color:#f92672>=</span> normalize_data(y_raw)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model, history <span style=color:#f92672>=</span> train_linear_regression(x_norm, y_norm)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 視覺化結果</span>
</span></span><span style=display:flex><span>plot_training_process(x_raw<span style=color:#f92672>.</span>flatten(), y_raw<span style=color:#f92672>.</span>flatten(), 
</span></span><span style=display:flex><span>                        x_norm<span style=color:#f92672>.</span>flatten(), y_norm<span style=color:#f92672>.</span>flatten(), history)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 輸出最終結果</span>
</span></span><span style=display:flex><span>final_w <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;w&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_b <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;b&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>final_loss <span style=color:#f92672>=</span> history[<span style=color:#e6db74>&#39;loss&#39;</span>][<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將係數轉換回原始尺度</span>
</span></span><span style=display:flex><span>w_raw <span style=color:#f92672>=</span> final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw)
</span></span><span style=display:flex><span>b_raw <span style=color:#f92672>=</span> (final_b <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>mean(y_raw) <span style=color:#f92672>-</span> 
</span></span><span style=display:flex><span>        final_w <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>std(y_raw) <span style=color:#f92672>*</span> np<span style=color:#f92672>.</span>mean(x_raw) <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>std(x_raw))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final equation: y = </span><span style=color:#e6db74>{</span>w_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>x + </span><span style=color:#e6db74>{</span>b_raw[<span style=color:#ae81ff>0</span>]<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;Final normalized loss: </span><span style=color:#e6db74>{</span>final_loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.6f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div><p><img alt=sample_with_gradient_descent loading=lazy src=/ai/AI/3_4/sample_with_gradient_descent.png></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/3_3/><span class=title>« 上一頁</span><br><span>[AI] 3-3. 使用 TensorFlow 與 Keras 函式庫</span>
</a><a class=next href=https://intervalrain.github.io/ai/3_5/><span class=title>下一頁 »</span><br><span>[AI] 3-5. 實作線性分類器</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2024 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>