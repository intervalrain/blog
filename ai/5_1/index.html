<!doctype html><html lang=zh-tw dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[AI] 普適化 | Rain Hu's Workspace</title>
<meta name=keywords content="AI"><meta name=description content="Introduction to generalization"><meta name=author content="Rain Hu"><link rel=canonical href=https://intervalrain.github.io/><meta name=google-site-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.662816b9df27c772d2b97c5f5f6bf4f2c5531051a330015f0ad4135736d0e56a.css integrity="sha256-ZigWud8nx3LSuXxfX2v08sVTEFGjMAFfCtQTVzbQ5Wo=" rel="preload stylesheet" as=style><link rel=icon href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=16x16 href=https://intervalrain.github.io/images/rain.png><link rel=icon type=image/png sizes=32x32 href=https://intervalrain.github.io/images/rain.png><link rel=apple-touch-icon href=https://intervalrain.github.io/images/rain.png><link rel=mask-icon href=https://intervalrain.github.io/images/rain.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=zh-tw href=https://intervalrain.github.io/ai/5_1/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.css integrity=sha384-zTROYFVGOfTw7JV7KUu8udsvW2fx4lWOsCEDqhBreBwlHI4ioVRtmIvEThzJHGET crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/katex.min.js integrity=sha384-GxNFqL3r9uRJQhR+47eDxuPoNE7yLftQM8LcxzgS4HT73tp970WS/wV5p8UzCOmb crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.13.18/dist/contrib/auto-render.min.js integrity=sha384-vZTG03m+2yp6N6BNi5iM4rW4oIwk5DfcNdFfxkk9ZWpDriOkXX8voJBFrAO7MpVl crossorigin=anonymous onload=renderMathInElement(document.body)></script><script src=https://utteranc.es/client.js repo=intervalrain.github.io issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script><meta property="og:url" content="https://intervalrain.github.io/ai/5_1/"><meta property="og:site_name" content="Rain Hu's Workspace"><meta property="og:title" content="[AI] 普適化"><meta property="og:description" content="Introduction to generalization"><meta property="og:locale" content="zh-tw"><meta property="og:type" content="article"><meta property="article:section" content="ai"><meta property="article:published_time" content="2025-01-12T17:52:09+08:00"><meta property="article:modified_time" content="2025-01-12T17:52:09+08:00"><meta property="article:tag" content="AI"><meta name=twitter:card content="summary"><meta name=twitter:title content="[AI] 普適化"><meta name=twitter:description content="Introduction to generalization"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"[AI] 普適化","item":"https://intervalrain.github.io/ai/5_1/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[AI] 普適化","name":"[AI] 普適化","description":"Introduction to generalization","keywords":["AI"],"articleBody":" 機器學習最重要的兩件事是：\n準確的模型評估 訓練次數與普適化之間的平衡 普適化(generalization) 普適化是機器學習的終極目標，什麼是普適化呢？首先要先解釋是什麼低度擬合(underfitting) 與 過度擬合(overfitting)。 觀察下方的圖，訓練集與驗證集在訓練初期，損失值都穩定下降，此時稱為 underfitting，代表神經網路尚未學習到資料中的共同特徵。經過一定時間後，驗證指標會開始停滯並開始變差，這代表模型開始發生 overfitting，代表模型已經額外學習了一些只有訓練集中的特徵，進而可能在面對新資料時造成干擾。而 穩健擬合(robust fit) 是 underfitting 與 overfitting 之間的點，代表最佳的 epochs。 下圖的黑線與就是 robust fit 的表現，綠線是 overfitting 的表現。 可以看到綠線在訓練集有很好的表現，但可能會在新的資料點進入時，有錯誤的判斷。 普適化就是找到一個面對所有資料都能有穩定且好的表現的模型。 overfitting overfitting 容易發生在 具有雜訊的資料 具有罕見特徵 標示錯誤的資料 如果訓練過程，模型針對這些離群值(outlier)進行學習，普適化表現自然會下降。 模糊特徵 然後並非所有雜訊都是由不準確性(特徵模糊/標示錯誤)產生的，當處理的問題本身就具備不確定性或模棱兩可時，就算是字跡清晰、標籤正確也可能是雜訊，特別是一些沒有明確界線的特徵。 就像是下面的三杯一樣的水，由不同的人來 label，也會 label 出不一樣的答案。 最有感的就是問卷量表，客戶滿意度 (CSAT) 調查問卷通常分為 1 (非常不滿意) 到 5 分 (非常滿意) 。每個人對於滿意度的給分都不一致，所以就容易產生差異。 穩健的模型會忽略訓練資料中個別的資料點，從眾數著眼。 罕見特徵(rare feature)與虛假關聯(spurious correlation) 罕見特徵: 通常是樣本中出現頻率極低的特徵，可能具有高辨識度或影響力，但也可能是噪音，需要小心解讀。\n事故發生的地點是某條高速公路上極少使用的臨時匝道。 事故發生時段是凌晨2點到3點，且伴隨濃霧天氣。 涉事車輛是一輛載有易燃化學品的大型貨車，並與一輛滿載乘客的大巴相撞。 如何影響機器學習： 特徵稀疏性： 這些罕見的特徵組合在訓練數據中可能只出現過1-2次，但其後果卻極為嚴重（多人傷亡），模型可能過度強調這些罕見情況作為高危因素。 過擬合風險： 如果模型只根據這些少量案例進行學習，可能無法有效處理未見過的場景（如不同的道路或車輛組合）。 虛假關聯: 則是數據之間表面上有相關性，但實際上缺乏因果關係，可能由於第三因素驅動或純粹的巧合。\n冰淇淋銷量與溺水事件： 描述：夏季冰淇淋銷量與溺水事件呈現高度正相關。 原因：兩者都與溫度升高相關，並非冰淇淋銷量導致溺水事件增加。 接下來來做一個實驗，我們在每一張數字中擴充維度，分別擴充 noise 與 zeros from tensorflow.keras.datasets import mnist import numpy as np (train_images, train_labels), (test_images, test_labels) = mnist.load_data() train_images = train_images.reshape((60000, 28*28)) train_images = train_images.astype(\"float32\") / 255 train_images_with_noise = np.concatenate( [train_images, np.random.random((len(train_images), 784))], axis=1 ) train_images_with_zeros = np.concatenate( [train_images, np.zeros((len(train_images), 784))], axis=1 ) 接著我們用這兩個資料集來進行 training，來觀察訓練後的結果： from tensorflow import keras from tensorflow.keras import layers def build_model(): model = keras.Sequential([ layers.Dense(512, activation=\"relu\"), layers.Dense(10, activation=\"softmax\") ]) model.compile( optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) return model model = build_model() history_noise = model.fit(train_images_with_noise, train_labels, epochs=10, batch_size=128, validation_split=0.2) history_zeros = model.fit(train_images_with_zeros, train_labels, epochs=10, batch_size=128, validation_split=0.2) 畫出損失曲線 import matplotlib.pyplot as plt val_acc_noise = history_noise.history[\"val_accuracy\"] val_acc_zeros = history_zeros.history[\"val_accurarc\"] epochs = range(1, 11) plt.plot(epochs, val_acc_noise, \"b-\", label=\"data with noise\") plt.plot(epochs, val_acc_zeros, \"r-\", label=\"data with zeros\") plt.title(\"Effect of noise channels on validtaion accuracy\") plt.xlabel(\"Epochs\") plt.ylabel(\"Validation accuracy\") plt.legend() 可見兩組資料擁有相同的有效特徵資訊，但所訓練出來的模型在驗證準確度卻有明顯的差異，這差距來自於虛假關聯：但加入的雜訊愈多，準確度就會愈低。 雜訊幾乎都會造成 overfitting，故若不確定各種特徵是有用或無用時，通常要在訓練前進行特徵挑選(feature selection)。例如先前在 IMDB 資料，便時採用前 10000 個常用字。 常用的方法還包括特量每個特徵對於任務關聯性的量測，如特徵與標籤之間的 MI(mutual information) 分數，只保留分數在一定門閾值以上的特徵，用以過濾雜訊。 ","wordCount":"204","inLanguage":"zh-tw","datePublished":"2025-01-12T17:52:09+08:00","dateModified":"2025-01-12T17:52:09+08:00","author":{"@type":"Person","name":"Rain Hu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://intervalrain.github.io/ai/5_1/"},"publisher":{"@type":"Organization","name":"Rain Hu's Workspace","logo":{"@type":"ImageObject","url":"https://intervalrain.github.io/images/rain.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://intervalrain.github.io/ accesskey=h title="Rain Hu's Workspace (Alt + H)"><img src=https://intervalrain.github.io/images/rain.png alt aria-label=logo height=35>Rain Hu's Workspace</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://intervalrain.github.io/search title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://intervalrain.github.io/aboutme title="About me"><span>About me</span></a></li><li><a href=https://intervalrain.github.io/archives title=Archives><span>Archives</span></a></li><li><a href=https://intervalrain.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://intervalrain.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://intervalrain.github.io/csharp/csharp title=C#><span>C#</span></a></li><li><a href=https://intervalrain.github.io/csindex title=CS><span>CS</span></a></li><li><a href=https://intervalrain.github.io/leetcode title=LeetCode><span>LeetCode</span></a></li><li><a href=https://intervalrain.github.io/ai title=AI><span>AI</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://intervalrain.github.io/>首頁</a></div><h1 class="post-title entry-hint-parent">[AI] 普適化</h1><div class=post-description>Introduction to generalization</div><div class=post-meta><span title='2025-01-12 17:52:09 +0800 +0800'>January 12, 2025</span>&nbsp;·&nbsp;1 分鐘&nbsp;·&nbsp;Rain Hu&nbsp;|&nbsp;<a href=https://github.com/intervalrain/intervalrain.github.io/tree/main/content//AI/5_1.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>目錄</span></summary><div class=inner><ul><li><a href=#%e6%99%ae%e9%81%a9%e5%8c%96generalization aria-label=普適化(generalization)>普適化(generalization)</a><ul><li><a href=#overfitting aria-label=overfitting>overfitting</a></li><li><a href=#%e6%a8%a1%e7%b3%8a%e7%89%b9%e5%be%b5 aria-label=模糊特徵>模糊特徵</a></li><li><a href=#%e7%bd%95%e8%a6%8b%e7%89%b9%e5%be%b5rare-feature%e8%88%87%e8%99%9b%e5%81%87%e9%97%9c%e8%81%afspurious-correlation aria-label="罕見特徵(rare feature)與虛假關聯(spurious correlation)">罕見特徵(rare feature)與虛假關聯(spurious correlation)</a></li></ul></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><blockquote><p>機器學習最重要的兩件事是：</p><ol><li>準確的模型評估</li><li>訓練次數與普適化之間的平衡</li></ol></blockquote><h2 id=普適化generalization>普適化(generalization)<a hidden class=anchor aria-hidden=true href=#普適化generalization>#</a></h2><ul><li>普適化是機器學習的終極目標，什麼是普適化呢？首先要先解釋是什麼<strong>低度擬合(underfitting)</strong> 與 <strong>過度擬合(overfitting)</strong>。<ul><li>觀察下方的圖，訓練集與驗證集在訓練初期，損失值都穩定下降，此時稱為 underfitting，代表神經網路尚未學習到資料中的共同特徵。經過一定時間後，驗證指標會開始停滯並開始變差，這代表模型開始發生 overfitting，代表模型已經額外學習了一些只有訓練集中的特徵，進而可能在面對新資料時造成干擾。而 <strong>穩健擬合(robust fit)</strong> 是 underfitting 與 overfitting 之間的點，代表最佳的 epochs。
<img alt=robust_fit loading=lazy src=/ai/AI/5_1/robust_fit.png></li><li>下圖的黑線與就是 robust fit 的表現，綠線是 overfitting 的表現。<ul><li>可以看到綠線在訓練集有很好的表現，但可能會在新的資料點進入時，有錯誤的判斷。</li><li><strong>普適化</strong>就是找到一個面對所有資料都能有穩定且好的表現的模型。
<img alt=overfitting loading=lazy src=/ai/AI/5_1/overfitting.webp></li></ul></li></ul></li></ul><h3 id=overfitting>overfitting<a hidden class=anchor aria-hidden=true href=#overfitting>#</a></h3><ul><li>overfitting 容易發生在<ol><li>具有雜訊的資料</li><li>具有罕見特徵</li><li>標示錯誤的資料</li></ol><ul><li>如果訓練過程，模型針對這些離群值(outlier)進行學習，普適化表現自然會下降。</li></ul></li></ul><h3 id=模糊特徵>模糊特徵<a hidden class=anchor aria-hidden=true href=#模糊特徵>#</a></h3><ul><li>然後並非所有雜訊都是由不準確性(特徵模糊/標示錯誤)產生的，當處理的問題本身就具備不確定性或模棱兩可時，就算是字跡清晰、標籤正確也可能是雜訊，特別是一些沒有明確界線的特徵。</li></ul><blockquote><p>就像是下面的三杯一樣的水，由不同的人來 label，也會 label 出不一樣的答案。
<img alt=wafer loading=lazy src=/ai/AI/5_1/water.jpeg></p></blockquote><ul><li>最有感的就是問卷量表，客戶滿意度 (CSAT) 調查問卷通常分為 1 (非常不滿意) 到 5 分 (非常滿意) 。每個人對於滿意度的給分都不一致，所以就容易產生差異。</li><li>穩健的模型會忽略訓練資料中個別的資料點，從眾數著眼。</li></ul><h3 id=罕見特徵rare-feature與虛假關聯spurious-correlation>罕見特徵(rare feature)與虛假關聯(spurious correlation)<a hidden class=anchor aria-hidden=true href=#罕見特徵rare-feature與虛假關聯spurious-correlation>#</a></h3><ul><li><p><strong>罕見特徵</strong>: 通常是樣本中出現頻率極低的特徵，可能具有高辨識度或影響力，但也可能是噪音，需要小心解讀。</p><ol><li>事故發生的地點是某條高速公路上極少使用的臨時匝道。</li><li>事故發生時段是凌晨2點到3點，且伴隨濃霧天氣。</li><li>涉事車輛是一輛載有易燃化學品的大型貨車，並與一輛滿載乘客的大巴相撞。</li></ol><ul><li>如何影響機器學習：<ul><li>特徵稀疏性： 這些罕見的特徵組合在訓練數據中可能只出現過1-2次，但其後果卻極為嚴重（多人傷亡），模型可能過度強調這些罕見情況作為高危因素。</li><li>過擬合風險： 如果模型只根據這些少量案例進行學習，可能無法有效處理未見過的場景（如不同的道路或車輛組合）。</li></ul></li></ul></li><li><p><strong>虛假關聯</strong>: 則是數據之間表面上有相關性，但實際上缺乏因果關係，可能由於第三因素驅動或純粹的巧合。</p><ul><li>冰淇淋銷量與溺水事件：<ul><li>描述：夏季冰淇淋銷量與溺水事件呈現高度正相關。</li><li>原因：兩者都與溫度升高相關，並非冰淇淋銷量導致溺水事件增加。</li></ul></li></ul></li><li><p>接下來來做一個實驗，我們在每一張數字中擴充維度，分別擴充 noise 與 zeros
<img alt=noise_test loading=lazy src=/ai/AI/5_1/noise_test.png></p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> mnist
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(train_images, train_labels), (test_images, test_labels) <span style=color:#f92672>=</span> mnist<span style=color:#f92672>.</span>load_data()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_images <span style=color:#f92672>=</span> train_images<span style=color:#f92672>.</span>reshape((<span style=color:#ae81ff>60000</span>, <span style=color:#ae81ff>28</span><span style=color:#f92672>*</span><span style=color:#ae81ff>28</span>))
</span></span><span style=display:flex><span>train_images <span style=color:#f92672>=</span> train_images<span style=color:#f92672>.</span>astype(<span style=color:#e6db74>&#34;float32&#34;</span>) <span style=color:#f92672>/</span> <span style=color:#ae81ff>255</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_images_with_noise <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(
</span></span><span style=display:flex><span>    [train_images, np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>random((len(train_images), <span style=color:#ae81ff>784</span>))], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>train_images_with_zeros <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>concatenate(
</span></span><span style=display:flex><span>    [train_images, np<span style=color:#f92672>.</span>zeros((len(train_images), <span style=color:#ae81ff>784</span>))], axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><ul><li>接著我們用這兩個資料集來進行 training，來觀察訓練後的結果：</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> layers
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>build_model</span>():
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> keras<span style=color:#f92672>.</span>Sequential([
</span></span><span style=display:flex><span>        layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>512</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;relu&#34;</span>),
</span></span><span style=display:flex><span>        layers<span style=color:#f92672>.</span>Dense(<span style=color:#ae81ff>10</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;softmax&#34;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>compile(
</span></span><span style=display:flex><span>        optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;rmsprop&#34;</span>,
</span></span><span style=display:flex><span>        loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;sparse_categorical_crossentropy&#34;</span>,
</span></span><span style=display:flex><span>        metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#34;accuracy&#34;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> build_model()
</span></span><span style=display:flex><span>history_noise <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(train_images_with_noise, train_labels, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, validation_split<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>)
</span></span><span style=display:flex><span>history_zeros <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(train_images_with_zeros, train_labels, epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>, validation_split<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>)
</span></span></code></pre></div><ul><li>畫出損失曲線</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>val_acc_noise <span style=color:#f92672>=</span> history_noise<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#34;val_accuracy&#34;</span>]
</span></span><span style=display:flex><span>val_acc_zeros <span style=color:#f92672>=</span> history_zeros<span style=color:#f92672>.</span>history[<span style=color:#e6db74>&#34;val_accurarc&#34;</span>]
</span></span><span style=display:flex><span>epochs <span style=color:#f92672>=</span> range(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>11</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_acc_noise, <span style=color:#e6db74>&#34;b-&#34;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data with noise&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(epochs, val_acc_zeros, <span style=color:#e6db74>&#34;r-&#34;</span>, label<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;data with zeros&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#34;Effect of noise channels on validtaion accuracy&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#34;Epochs&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#34;Validation accuracy&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span></code></pre></div><p><img alt=loss_curve loading=lazy src=/ai/AI/5_1/loss_curve.png></p><ul><li>可見兩組資料擁有相同的有效特徵資訊，但所訓練出來的模型在驗證準確度卻有明顯的差異，這差距來自於虛假關聯：但加入的雜訊愈多，準確度就會愈低。</li><li>雜訊幾乎都會造成 overfitting，故若不確定各種特徵是有用或無用時，通常要在訓練前進行特徵挑選(feature selection)。例如先前在 IMDB 資料，便時採用前 10000 個常用字。<ul><li>常用的方法還包括特量每個特徵對於任務關聯性的量測，如特徵與標籤之間的 MI(mutual information) 分數，只保留分數在一定門閾值以上的特徵，用以過濾雜訊。</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://intervalrain.github.io/tags/ai/>AI</a></li></ul><nav class=paginav><a class=prev href=https://intervalrain.github.io/ai/4_3/><span class=title>« 上一頁</span><br><span>[AI] 迴歸問題</span>
</a><a class=next href=https://intervalrain.github.io/ai/5_2/><span class=title>下一頁 »</span><br><span>[AI] 評估模型</span></a></nav></footer><script src=https://utteranc.es/client.js repo=Reid00/hugo-blog-talks issue-term=pathname label=Comment theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2025 <a href=https://intervalrain.github.io/>Rain Hu's Workspace</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="複製";function s(){t.innerHTML="已複製！",setTimeout(()=>{t.innerHTML="複製"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>